{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dca4ed1",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515c0e2",
   "metadata": {},
   "source": [
    "This Algorithm creates a prediction function that re-creates the mappings and trains the model inside of it at every run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b024fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "def save_pkl(object_to_store, name: str, path: str = './'):\n",
    "    \"\"\"\n",
    "    :param object_to_store: object to pickle\n",
    "    :param name: name to save pickle\n",
    "    :param path: path to save pickle\n",
    "    \"\"\"\n",
    "    with open('{}/{}.pkl'.format(path, name), 'wb') as file:\n",
    "        pickle.dump(object_to_store, file)\n",
    "\n",
    "def local_load_pkl_model(model_name: str, path: str = './'):\n",
    "    \"\"\"\n",
    "    :param model_name: name of pickled model to load\n",
    "    :param path: path to the pickled model\n",
    "    :return: the pickle model\n",
    "    \"\"\"\n",
    "    with open('{}/{}.pkl'.format(path, model_name), 'rb') as file:\n",
    "        b = pickle.load(file)\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e772a06",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e759e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastavro import parse_schema, json_reader\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a210b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {}\n",
    "schema = {\n",
    "    'doc': 'Article collection schema',\n",
    "    'name': 'Article',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 'article_id', 'type': 'int'},\n",
    "        {'name': 'product_code', 'type': 'int'},\n",
    "        {'name': 'prod_name', 'type': 'string'},\n",
    "        {'name': 'product_type_no', 'type': 'int'},\n",
    "        {'name': 'product_type_name', 'type': 'string'},\n",
    "        {'name': 'product_group_name', 'type': 'string'},\n",
    "        {'name': 'graphical_appearance_no', 'type': 'int'},\n",
    "        {'name': 'graphical_appearance_name', 'type': 'string'},\n",
    "        {'name': 'colour_group_code', 'type': 'int'},\n",
    "        {'name': 'colour_group_name', 'type': 'string'},\n",
    "        {'name': 'perceived_colour_value_id', 'type': 'int'},\n",
    "        {'name': 'perceived_colour_value_name', 'type': 'string'},\n",
    "        {'name': 'perceived_colour_master_id', 'type': 'int'},\n",
    "        {'name': 'perceived_colour_master_name', 'type': 'string'},\n",
    "        {'name': 'department_no', 'type': 'int'},\n",
    "        {'name': 'department_name', 'type': 'string'},\n",
    "        {'name': 'index_code', 'type': 'string'},\n",
    "        {'name': 'index_name', 'type': 'string'},\n",
    "        {'name': 'index_group_no', 'type': 'int'},\n",
    "        {'name': 'index_group_name', 'type': 'string'},\n",
    "        {'name': 'section_no', 'type': 'int'},\n",
    "        {'name': 'section_name', 'type': 'string'},\n",
    "        {'name': 'garment_group_no', 'type': 'int'},\n",
    "        {'name': 'garment_group_name', 'type': 'string'},\n",
    "        {'name': 'detail_desc', 'type': 'string'},\n",
    "        {'name': 'article_url', 'type': 'string'}\n",
    "    ],\n",
    "}\n",
    "schemas['article'] = parse_schema(schema)\n",
    "\n",
    "\n",
    "schema = {\n",
    "    'doc': 'Customer collection schema',\n",
    "    'name': 'Customer',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 'customer_id', 'type': 'string'},\n",
    "        {'name': 'FN', 'type': 'string'},\n",
    "        {'name': 'Active', 'type': 'string'},\n",
    "        {'name': 'club_member_status', 'type': 'string'},\n",
    "        {'name': 'fashion_news_frequency', 'type': 'string'},\n",
    "        {'name': 'age', 'type': 'int'},\n",
    "        {'name': 'postal_code', 'type': 'string'}\n",
    "    ],\n",
    "}\n",
    "schemas['customer'] = parse_schema(schema)\n",
    "\n",
    "\n",
    "schema = {\n",
    "    'doc': 'Transaction collection schema',\n",
    "    'name': 'Transaction',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 't_dat', 'type': 'string'},\n",
    "        {'name': 'customer_id', 'type': 'string'},\n",
    "        {'name': 'article_id', 'type': 'int'},\n",
    "        {'name': 'price', 'type': 'int'},\n",
    "        {'name': 'sales_channel_id', 'type': 'long'}\n",
    "    ],\n",
    "}\n",
    "schemas['transaction'] = parse_schema(schema)\n",
    "\n",
    "def load(collection):\n",
    "    records = []\n",
    "    with open('./collections/' + collection + '.json', 'r') as fo:\n",
    "        avro_reader = json_reader(fo, schemas[collection])\n",
    "        for record in avro_reader:\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "data = {\n",
    "    'org_id': 1,  # organization id (added by the \"loader\")\n",
    "    'article': [load('article')],  # bonuses for this user\n",
    "    'customer': [load('customer')],  # payments for this user\n",
    "    'transaction': [load('transaction')],  # games for this user\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac6af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3facc261",
   "metadata": {},
   "source": [
    "## Create feature_names.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "162f0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['customer','transaction','article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5543b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use save_pkl function to store feature_names object\n",
    "save_pkl(object_to_store = feature_names, name = \"feature_names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857f118",
   "metadata": {},
   "source": [
    "## Create prediction.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5f8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(data):\n",
    "    \n",
    "    import os\n",
    "    import tqdm\n",
    "    import ast\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from lightfm import LightFM\n",
    "    from lightfm.data import Dataset\n",
    "\n",
    "    # Import LightFM's evaluation metrics\n",
    "    from lightfm.evaluation import precision_at_k\n",
    "\n",
    "    %matplotlib inline\n",
    "    SEED = 42\n",
    "    np.random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    \n",
    "    \n",
    "    ## Prepare Dataset\n",
    "\n",
    "    data = DataFrame(data)\n",
    "    # Get customer data\n",
    "    customer_df1 = data['customer'].astype(str).dropna().apply(ast.literal_eval)\n",
    "    customer_df2 = pd.concat([pd.DataFrame(x) for x in customer_df1], keys=customer_df1.index)\n",
    "    customer = data[['org_id']].join(customer_df2.reset_index(level=1, drop=True)).reset_index(drop=True)\n",
    "\n",
    "    # Get article data\n",
    "    article_df1 = data['article'].astype(str).dropna().apply(ast.literal_eval)\n",
    "    article_df2 = pd.concat([pd.DataFrame(x) for x in article_df1], keys=article_df1.index)\n",
    "    article = data[['org_id']].join(article_df2.reset_index(level=1, drop=True)).reset_index(drop=True)\n",
    "    article[\"article_id\"] =  article[\"article_id\"].astype(str)\n",
    "\n",
    "    # Get transaction data\n",
    "    transaction_df1 = data['transaction'].astype(str).dropna().apply(ast.literal_eval)\n",
    "    transaction_df2 = pd.concat([pd.DataFrame(x) for x in transaction_df1], keys=transaction_df1.index)\n",
    "    transaction = data[['org_id']].join(transaction_df2.reset_index(level=1, drop=True)).reset_index(drop=True)\n",
    "    transaction[\"t_dat\"] =  pd.to_datetime(transaction[\"t_dat\"])\n",
    "    transaction[\"article_id\"] =  transaction[\"article_id\"].astype(str)\n",
    "\n",
    "\n",
    "    dataset = Dataset()\n",
    "    dataset.fit(users=customer['customer_id'].unique(), \n",
    "                items=article['article_id'].unique())\n",
    "\n",
    "    num_users, num_topics = dataset.interactions_shape()\n",
    "    print(f'Number of users: {num_users}, Number of topics: {num_topics}.')\n",
    "\n",
    "    #train_set = train[(train.t_dat>='2020-8-26')&(train.t_dat<='2020-9-15')]\n",
    "    train_set = transaction\n",
    "\n",
    "    (interactions, weights) = dataset.build_interactions(train_set.iloc[:, 2:4].values)\n",
    "\n",
    "    # default number of recommendations\n",
    "    K = 10\n",
    "    EPOCHS = 1\n",
    "\n",
    "    # model learning rate\n",
    "    LEARNING_RATE = 0.25\n",
    "    # no of latent factors\n",
    "    NO_COMPONENTS = 20\n",
    "\n",
    "    # no of threads to fit model\n",
    "    NO_THREADS = 32\n",
    "    # regularisation for both user and item features\n",
    "    ITEM_ALPHA=1e-6\n",
    "    USER_ALPHA=1e-6\n",
    "\n",
    "\n",
    "    light_fm = LightFM(loss='warp', no_components=NO_COMPONENTS, \n",
    "                     learning_rate=LEARNING_RATE,                 \n",
    "                     random_state=np.random.RandomState(SEED))\n",
    "    light_fm.fit(interactions=interactions, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "    #Get the mappings\n",
    "    '''\n",
    "    uid = mapping from customer_id to model equivalent user_id\n",
    "    iid = mapping from article_id to  model equivalent article_id\n",
    "    '''\n",
    "    uid_map, ufeature_map, iid_map, ifeature_map = dataset.mapping() \n",
    "    '''\n",
    "    create inverse mappings\n",
    "    '''\n",
    "    inv_uid_map = {v:k for k, v in uid_map.items()}\n",
    "    inv_iid_map = {v:k for k, v in iid_map.items()}\n",
    "\n",
    "    #convert submission user_id and article_id to model equivalent user_id and article_id\n",
    "\n",
    "    test_X = customer.customer_id.values.tolist()\n",
    "    lfn_user = lambda x: uid_map[x]\n",
    "    \n",
    "    test_X_m = [lfn_user(tx) for tx in test_X]\n",
    "    print(len(test_X_m))\n",
    "\n",
    "\n",
    "    customer_ids = []\n",
    "    preds = []\n",
    "\n",
    "    for usr_ in tqdm.tqdm(test_X_m, total = len(test_X_m)):\n",
    "        m_opt = light_fm.predict(np.array([usr_] * len(iid_map)), np.array(list(iid_map.values())))\n",
    "        pred = np.argsort(-m_opt)[:K]\n",
    "        customer_ids.append(inv_uid_map[usr_])\n",
    "        preds.append(' '.join([inv_iid_map[p] for p in pred]).strip())\n",
    "        #break\n",
    "    \n",
    "    customer_ids = np.array(customer_ids).reshape(-1, 1)\n",
    "    preds = np.array(preds).reshape(-1, 1)\n",
    "\n",
    "    final_preds = pd.DataFrame(data=np.concatenate((customer_ids, preds), axis=1).reshape(-1, 2), columns=['customer_id', 'prediction'])\n",
    "    return final_preds\n",
    "\n",
    "save_pkl(object_to_store = prediction, name = \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235d5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9fcc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af58a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e10f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf3d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "579c49cf",
   "metadata": {},
   "source": [
    "# Predictions ☂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec95693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "\n",
    "def save_pkl(object_to_store, name: str, path: str = './'):\n",
    "    \"\"\"\n",
    "    :param object_to_store: object to pickle\n",
    "    :param name: name to save pickle\n",
    "    :param path: path to save pickle\n",
    "    \"\"\"\n",
    "    with open('{}/{}.pkl'.format(path, name), 'wb') as file:\n",
    "        pickle.dump(object_to_store, file)\n",
    "\n",
    "def local_load_pkl_model(model_name: str, path: str = './'):\n",
    "    \"\"\"\n",
    "    :param model_name: name of pickled model to load\n",
    "    :param path: path to the pickled model\n",
    "    :return: the pickle model\n",
    "    \"\"\"\n",
    "    with open('{}/{}.pkl'.format(path, model_name), 'rb') as file:\n",
    "        b = pickle.load(file)\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e0e72",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad27a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastavro import parse_schema, json_reader\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978c64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f0e895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading collection features: ['customer', 'transaction', 'article']\n"
     ]
    }
   ],
   "source": [
    "feature_names = local_load_pkl_model(model_name=f'feature_names')\n",
    "print(f'Loading collection features: {feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92772b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e5660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = {}\n",
    "schema = {\n",
    "    'doc': 'Article collection schema',\n",
    "    'name': 'Article',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 'article_id', 'type': 'int'},\n",
    "        {'name': 'product_code', 'type': 'int'},\n",
    "        {'name': 'prod_name', 'type': 'string'},\n",
    "        {'name': 'product_type_no', 'type': 'int'},\n",
    "        {'name': 'product_type_name', 'type': 'string'},\n",
    "        {'name': 'product_group_name', 'type': 'string'},\n",
    "        {'name': 'graphical_appearance_no', 'type': 'int'},\n",
    "        {'name': 'graphical_appearance_name', 'type': 'string'},\n",
    "        {'name': 'colour_group_code', 'type': 'int'},\n",
    "        {'name': 'colour_group_name', 'type': 'string'},\n",
    "        {'name': 'perceived_colour_value_id', 'type': 'int'},\n",
    "        {'name': 'perceived_colour_value_name', 'type': 'string'},\n",
    "        {'name': 'perceived_colour_master_id', 'type': 'int'},\n",
    "        {'name': 'perceived_colour_master_name', 'type': 'string'},\n",
    "        {'name': 'department_no', 'type': 'int'},\n",
    "        {'name': 'department_name', 'type': 'string'},\n",
    "        {'name': 'index_code', 'type': 'string'},\n",
    "        {'name': 'index_name', 'type': 'string'},\n",
    "        {'name': 'index_group_no', 'type': 'int'},\n",
    "        {'name': 'index_group_name', 'type': 'string'},\n",
    "        {'name': 'section_no', 'type': 'int'},\n",
    "        {'name': 'section_name', 'type': 'string'},\n",
    "        {'name': 'garment_group_no', 'type': 'int'},\n",
    "        {'name': 'garment_group_name', 'type': 'string'},\n",
    "        {'name': 'detail_desc', 'type': 'string'},\n",
    "        {'name': 'article_url', 'type': 'string'}\n",
    "    ],\n",
    "}\n",
    "schemas['article'] = parse_schema(schema)\n",
    "\n",
    "\n",
    "schema = {\n",
    "    'doc': 'Customer collection schema',\n",
    "    'name': 'Customer',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 'customer_id', 'type': 'string'},\n",
    "        {'name': 'FN', 'type': 'string'},\n",
    "        {'name': 'Active', 'type': 'string'},\n",
    "        {'name': 'club_member_status', 'type': 'string'},\n",
    "        {'name': 'fashion_news_frequency', 'type': 'string'},\n",
    "        {'name': 'age', 'type': 'int'},\n",
    "        {'name': 'postal_code', 'type': 'string'}\n",
    "    ],\n",
    "}\n",
    "schemas['customer'] = parse_schema(schema)\n",
    "\n",
    "\n",
    "schema = {\n",
    "    'doc': 'Transaction collection schema',\n",
    "    'name': 'Transaction',\n",
    "    'namespace': 'test',\n",
    "    'type': 'record',\n",
    "    'fields': [\n",
    "        {'name': 't_dat', 'type': 'string'},\n",
    "        {'name': 'customer_id', 'type': 'string'},\n",
    "        {'name': 'article_id', 'type': 'int'},\n",
    "        {'name': 'price', 'type': 'int'},\n",
    "        {'name': 'sales_channel_id', 'type': 'long'}\n",
    "    ],\n",
    "}\n",
    "schemas['transaction'] = parse_schema(schema)\n",
    "\n",
    "def load(collection):\n",
    "    records = []\n",
    "    with open('./collections/' + collection + '.json', 'r') as fo:\n",
    "        avro_reader = json_reader(fo, schemas[collection])\n",
    "        for record in avro_reader:\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "data = {\n",
    "    'org_id': 1,  # organization id (added by the \"loader\")\n",
    "    'article': [load('article')],  # bonuses for this user\n",
    "    'customer': [load('customer')],  # payments for this user\n",
    "    'transaction': [load('transaction')],  # games for this user\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fd9452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prediction function\n"
     ]
    }
   ],
   "source": [
    "prediction_function = local_load_pkl_model(model_name=f'prediction')\n",
    "print('Loading prediction function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd9d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supply predcition function with collection data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonycini/opt/anaconda3/lib/python3.9/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 10001, Number of topics: 50000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|███████████████████████████████████████████████████████████████████████████████████▏                                          | 6598/10001 [01:25<00:40, 83.60it/s]"
     ]
    }
   ],
   "source": [
    "print('Supply predcition function with collection data')\n",
    "prediction = prediction_function(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862860c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4720fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bf623de",
   "metadata": {},
   "source": [
    "## requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad056c3",
   "metadata": {},
   "source": [
    "The final step would be to provide us with all the libraries that those pkl objects are using. So for example in the model code cell you are importing **lightgbm** so we will need to know about this library. The **requirements.txt** correspond to used libraries and packages for your enviornment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fac158",
   "metadata": {},
   "source": [
    "lightgbm <br>\n",
    "scikit-learn <br>\n",
    "pandas <br>\n",
    "dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4672aece",
   "metadata": {},
   "source": [
    "**So for the above examples it's pretty straight forward and the contents of this file are the above 3 libraries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c3d83",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e555e",
   "metadata": {},
   "source": [
    "Note that if you have some version dependency of a specific library make sure to define the specific version\n",
    "required. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76863f",
   "metadata": {},
   "source": [
    "lightgbm == 3.3.2 <br>\n",
    "scikit-learn == 0.24.2 <br>\n",
    "pandas == 1.3.4 <br>\n",
    "dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616fa95",
   "metadata": {},
   "source": [
    "##  IMP Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814df0fb",
   "metadata": {},
   "source": [
    "Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f98615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560064b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
